{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS 109A/STAT 121A/AC 209A/CSCI E-109A: Homework 4\n",
    "# Regularization, High Dimensionality, PCA\n",
    "\n",
    "\n",
    "**Harvard University**<br/>\n",
    "**Fall 2017**<br/>\n",
    "**Instructors**: Pavlos Protopapas, Kevin Rader, Rahul Dave, Margo Levine\n",
    "\n",
    "---\n",
    "\n",
    "### INSTRUCTIONS\n",
    "\n",
    "- To submit your assignment follow the instructions given in canvas.\n",
    "- Restart the kernel and run the whole notebook again before you submit. \n",
    "- Do not include your name(s) in the notebook even if you are submitting as a group. \n",
    "- If you submit individually and you have worked with someone, please include the name of your [one] partner below. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your partner's name (if you submit separately): Walter Thornton and Dwayne Kennemore\n",
    "\n",
    "Enrollment Status (109A, 121A, 209A, or E109A): E109A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.decomposition import PCA\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.api import OLS\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "%matplotlib inline\n",
    "pd.set_option('display.max_rows', 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuing Bike Sharing Usage Data\n",
    "\n",
    "In this homework, we will focus on multiple linear regression, regularization, dealing with high dimensionality, and PCA. We will continue to build regression models for the Capital Bikeshare program in Washington D.C.  See Homework 3 for more information about the data.\n",
    "\n",
    "*Note: please make sure you use all the processed data from HW 3 Part (a)...you make want to save the data set on your computer and reread the csv/json file here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# open the files\n",
    "train_data = pd.read_csv('train_data_hw4.csv', sep=\",\", header=0)\n",
    "test_data = pd.read_csv('test_data_hw4.csv', sep=\",\", header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.rename(index=str, columns={\"count\": \"rentals\"}, inplace=True)\n",
    "test_data.rename(index=str, columns={\"count\": \"rentals\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train_data['rentals'].values\n",
    "X_train = train_data[['holiday', 'workingday', 'temp', 'atemp', 'humidity', 'windspeed', 'season_2', \n",
    "                 'season_3', 'season_4', 'month_2', 'month_3', 'month_4', 'month_5', 'month_6',\n",
    "                 'month_7', 'month_8', 'month_9', 'month_10', 'month_11', 'month_12', 'day_of_week_1', \n",
    "                 'day_of_week_2', 'day_of_week_3', 'day_of_week_4', 'day_of_week_5', 'day_of_week_6', 'weather_2', \n",
    "                 'weather_3']].values\n",
    "\n",
    "y_test = test_data['rentals'].values\n",
    "X_test = test_data[['holiday', 'workingday', 'temp', 'atemp', 'humidity', 'windspeed', 'season_2', \n",
    "                 'season_3', 'season_4', 'month_2', 'month_3', 'month_4', 'month_5', 'month_6',\n",
    "                 'month_7', 'month_8', 'month_9', 'month_10', 'month_11', 'month_12', 'day_of_week_1', \n",
    "                 'day_of_week_2', 'day_of_week_3', 'day_of_week_4', 'day_of_week_5', 'day_of_week_6', 'weather_2', \n",
    "                 'weather_3']].values\n",
    "train_data = train_data.drop('Unnamed: 0', 1)\n",
    "train_data = train_data.drop('Unnamed: 0.1', 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(331, 28) (331, 1) (400, 28) (400, 1)\n"
     ]
    }
   ],
   "source": [
    "y_train = y_train.reshape(len(y_train), 1)\n",
    "y_test = y_test.reshape(len(y_test), 1)\n",
    "\n",
    "print(X_train.shape, y_train.shape, X_test.shape , y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>holiday</th>\n",
       "      <th>workingday</th>\n",
       "      <th>temp</th>\n",
       "      <th>atemp</th>\n",
       "      <th>humidity</th>\n",
       "      <th>windspeed</th>\n",
       "      <th>rentals</th>\n",
       "      <th>season_2</th>\n",
       "      <th>season_3</th>\n",
       "      <th>season_4</th>\n",
       "      <th>month_2</th>\n",
       "      <th>month_3</th>\n",
       "      <th>month_4</th>\n",
       "      <th>month_5</th>\n",
       "      <th>month_6</th>\n",
       "      <th>month_7</th>\n",
       "      <th>month_8</th>\n",
       "      <th>month_9</th>\n",
       "      <th>month_10</th>\n",
       "      <th>month_11</th>\n",
       "      <th>month_12</th>\n",
       "      <th>day_of_week_1</th>\n",
       "      <th>day_of_week_2</th>\n",
       "      <th>day_of_week_3</th>\n",
       "      <th>day_of_week_4</th>\n",
       "      <th>day_of_week_5</th>\n",
       "      <th>day_of_week_6</th>\n",
       "      <th>weather_2</th>\n",
       "      <th>weather_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.341801</td>\n",
       "      <td>-1.363792</td>\n",
       "      <td>-0.500703</td>\n",
       "      <td>0.040945</td>\n",
       "      <td>3830.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.431146</td>\n",
       "      <td>-1.665877</td>\n",
       "      <td>0.132958</td>\n",
       "      <td>2.036025</td>\n",
       "      <td>2114.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.695943</td>\n",
       "      <td>1.757749</td>\n",
       "      <td>-0.457103</td>\n",
       "      <td>-0.523392</td>\n",
       "      <td>915.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.805728</td>\n",
       "      <td>-0.759623</td>\n",
       "      <td>-0.997746</td>\n",
       "      <td>0.986696</td>\n",
       "      <td>4322.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.981180</td>\n",
       "      <td>0.952190</td>\n",
       "      <td>0.441062</td>\n",
       "      <td>0.311061</td>\n",
       "      <td>6591.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  Unnamed: 0.1  holiday  workingday      temp     atemp  \\\n",
       "0           0             0      0.0         1.0 -1.341801 -1.363792   \n",
       "1           1             1      0.0         1.0 -1.431146 -1.665877   \n",
       "2           2             2      0.0         1.0  1.695943  1.757749   \n",
       "3           3             3      0.0         1.0 -0.805728 -0.759623   \n",
       "4           4             4      0.0         0.0  0.981180  0.952190   \n",
       "\n",
       "   humidity  windspeed  rentals  season_2  season_3  season_4  month_2  \\\n",
       "0 -0.500703   0.040945   3830.0         0         0         0        1   \n",
       "1  0.132958   2.036025   2114.0         0         0         0        0   \n",
       "2 -0.457103  -0.523392    915.0         1         0         0        0   \n",
       "3 -0.997746   0.986696   4322.0         0         0         0        1   \n",
       "4  0.441062   0.311061   6591.0         1         0         0        0   \n",
       "\n",
       "   month_3  month_4  month_5  month_6  month_7  month_8  month_9  month_10  \\\n",
       "0        0        0        0        0        0        0        0         0   \n",
       "1        0        0        0        0        0        0        0         0   \n",
       "2        0        0        0        1        0        0        0         0   \n",
       "3        0        0        0        0        0        0        0         0   \n",
       "4        0        0        1        0        0        0        0         0   \n",
       "\n",
       "   month_11  month_12  day_of_week_1  day_of_week_2  day_of_week_3  \\\n",
       "0         0         0              0              0              0   \n",
       "1         0         1              0              0              0   \n",
       "2         0         0              0              0              0   \n",
       "3         0         0              1              0              0   \n",
       "4         0         0              0              0              0   \n",
       "\n",
       "   day_of_week_4  day_of_week_5  day_of_week_6  weather_2  weather_3  \n",
       "0              1              0              0          0          0  \n",
       "1              1              0              0          1          0  \n",
       "2              1              0              0          1          0  \n",
       "3              0              0              0          0          0  \n",
       "4              0              0              0          0          0  "
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part (f): Regularization/Penalization Methods\n",
    "\n",
    "As an alternative to selecting a subset of predictors and fitting a regression model on the subset, one can fit a linear regression model on all predictors, but shrink or regularize the coefficient estimates to make sure that the model does not \"overfit\" the training set. \n",
    "\n",
    "Use the following regularization techniques to fit linear models to the training set:\n",
    "- Ridge regression\n",
    "- Lasso regression\n",
    "    \n",
    "You may choose the shrikage parameter $\\lambda$ from the set $\\{10^{-5}, 10^{-4},...,10^{4},10^{5}\\}$ using cross-validation. In each case, \n",
    "\n",
    "- How do the estimated coefficients compare to or differ from the coefficients estimated by a plain linear regression (without shrikage penalty) in Part (b) fropm HW 3? Is there a difference between coefficients estimated by the two shrinkage methods? If so, give an explantion for the difference.\n",
    "- List the predictors that are assigned a coefficient value close to 0 (say < 1e-10) by the two methods. How closely do these predictors match the redundant predictors (if any) identified in Part (c) from HW 3?\n",
    "- Is there a difference in the way Ridge and Lasso regression assign coefficients to the predictors `temp` and `atemp`? If so, explain the reason for the difference.\n",
    "\n",
    "We next analyze the performance of the two shrinkage methods for different training sample sizes:\n",
    "- Generate random samples of sizes 100, 150, ..., 400 from the training set. You may use the following code to draw a random sample of a specified size from the training set:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "*Analysis*\n",
    "\n",
    "\n",
    "Only thee lasso regression had values close to 0. These seven are indicated below in the list of Lasso coefficients. \n",
    "The ridge regression yielded no such results. Some of these coincide with the predictors from hw3 that were deemed unnecessary. Monthe 2, month 5, holiday, month 8, month 12, day_of_week_4 and weather 2 all recieved 0's on the lasso regression. This makes sense, since these predictors convey similar information as seasons, weather etc.\n",
    "Ridge regression seems to give atemp and temp the same coeff, with opposong signs, suggesting that they almost 'cancel each other out'. Lasso gives a larger negative weight to atemp.\n",
    "Since lasso 0's out a number of of predictors and simplifies the model. Both reduce coefficients, but lasso makes the model itself less complex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:484: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/opt/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:484: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/opt/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:484: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/opt/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:484: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/opt/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:484: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/opt/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:484: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/opt/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:484: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/opt/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:484: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/opt/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:484: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/opt/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:484: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/opt/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:484: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/opt/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:484: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/opt/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:484: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/opt/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:484: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/opt/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:484: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/opt/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:484: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/opt/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:484: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/opt/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:484: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/opt/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:484: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/opt/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:484: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/opt/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:484: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/opt/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:484: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/opt/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:484: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/opt/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:484: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/opt/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:484: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/opt/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:484: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/opt/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:484: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/opt/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:484: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/opt/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:484: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/opt/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:484: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/opt/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:484: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/opt/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:484: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/opt/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:484: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/opt/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:484: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/opt/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:484: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/opt/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:484: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:484: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/opt/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:484: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best ridge model has alpha= 10\n",
      "The best ridge model has score= 0.436358716542\n",
      "The best lasso model has alpha= 10\n",
      "The best lasso model has score= 0.415939790448\n"
     ]
    }
   ],
   "source": [
    "alpha_values = [10**-5, 10**-4, 10**-3, 10**-2, 10**-1, 10**0, 10**1, 10**2, 10**3, 10**4, 10**5]\n",
    "ridge_models = {}\n",
    "lasso_models = {}\n",
    "\n",
    "for alpha_val in alpha_values:\n",
    "\n",
    "    # build the ridge and lasso regression model with specified lambda, ie, alpha\n",
    "    ridge_reg = Ridge(alpha = alpha_val)\n",
    "    lasso_reg = Lasso(alpha = alpha_val)\n",
    "\n",
    "    # cross validate\n",
    "    scores_r = cross_val_score(ridge_reg, X_train, y_train, cv=10)\n",
    "    score_r = np.mean(scores_r)\n",
    "    temp_dict_r = {alpha_val : score_r}\n",
    "    ridge_models.update(temp_dict_r)\n",
    "    \n",
    "    scores_l = cross_val_score(lasso_reg, X_train, y_train, cv=10)\n",
    "    score_l = np.mean(scores_l)\n",
    "    temp_dict_l = {alpha_val : score_l}\n",
    "    lasso_models.update(temp_dict_l)\n",
    "\n",
    "\n",
    "\n",
    "best_lasso_alpha = max(lasso_models, key=lasso_models.get)\n",
    "best_lasso_score =lasso_models.get(best_lasso_alpha)\n",
    "\n",
    "best_ridge_alpha = max(ridge_models, key=ridge_models.get)\n",
    "best_ridge_score =ridge_models.get(best_ridge_alpha)\n",
    "\n",
    "print('The best ridge model has alpha=', best_ridge_alpha)\n",
    "print('The best ridge model has score=', best_ridge_score)\n",
    "\n",
    "print('The best lasso model has alpha=', best_lasso_alpha)\n",
    "print('The best lasso model has score=', best_lasso_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge Beta0 is: [ 4001.41604352]\n",
      "Ridge Betas are: [[-158.55971554  217.45335409  682.80825822  553.88994683 -567.72191933\n",
      "  -266.41311936  393.17255806  172.1792805   761.78543763 -115.1218662\n",
      "    88.89050681  369.53564813  133.37245386 -312.37487384 -529.54451686\n",
      "   -89.29404068  676.88686869  503.42304275  159.09586506 -100.6582173\n",
      "  -130.58113365 -125.49534264  124.31796857   64.47904074  126.17310553\n",
      "   303.9886922    20.28051056 -676.14388831]]\n",
      "Lasso Beta0 is: [ 3957.97842992]\n",
      "Lasso Betas are: [   -0.           277.51867781   854.72446879   399.37258548  -555.84062186\n",
      "  -254.36848284   543.88126524   113.79366043   898.94221619    -0.\n",
      "    13.91707518   312.53427367     0.          -354.59953274  -496.15246234\n",
      "    -0.           834.58471461   483.88131207    68.02103714    -0.\n",
      "  -178.96330537  -129.13772753    17.59586395     0.            11.33513761\n",
      "   309.9496593     -0.         -1058.29982989]\n"
     ]
    }
   ],
   "source": [
    "# fit best validated Ridge and lasso models\n",
    "\n",
    "ridge_reg = Ridge(alpha = best_ridge_alpha)\n",
    "lasso_reg = Lasso(alpha = best_lasso_alpha)\n",
    "\n",
    "#fit the model to training data\n",
    "ridge_reg.fit(X_train, y_train)  \n",
    "lasso_reg.fit(X_train, y_train)\n",
    "\n",
    "#save the beta coefficients\n",
    "beta0_ridge = ridge_reg.intercept_\n",
    "betas_ridge = ridge_reg.coef_\n",
    "\n",
    "#save the beta coefficients\n",
    "beta0_lasso = lasso_reg.intercept_\n",
    "betas_lasso = lasso_reg.coef_\n",
    "\n",
    "#make predictions everywhere\n",
    "ypredict_ridge = ridge_reg.predict(X_train)\n",
    "ypredict_lasso = lasso_reg.predict(X_train)\n",
    "\n",
    "print('Ridge Beta0 is:', beta0_ridge)\n",
    "print('Ridge Betas are:', betas_ridge)\n",
    "\n",
    "print('Lasso Beta0 is:', beta0_lasso)\n",
    "print('Lasso Betas are:', betas_lasso)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#--------  sample\n",
    "# A function to select a random sample of size k from the training set\n",
    "# Input: \n",
    "#      x (n x d array of predictors in training data)\n",
    "#      y (n x 1 array of response variable vals in training data)\n",
    "#      k (size of sample) \n",
    "# Return: \n",
    "#      chosen sample of predictors and responses\n",
    "\n",
    "def sample(x, y, k):\n",
    "    n = x.shape[0] # No. of training points\n",
    "    \n",
    "    # Choose random indices of size 'k'\n",
    "    subset_ind = np.random.choice(np.arange(n), k)\n",
    "    \n",
    "    # Get predictors and reponses with the indices\n",
    "    x_subset = x[subset_ind, :]\n",
    "    y_subset = y[subset_ind]\n",
    "    \n",
    "    return (x_subset, y_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'LinearRegression' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-d85a096380c6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msimpl_reg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLinearRegression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0msample_sizes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m150\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m200\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m250\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m300\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m350\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m400\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m columns = ['sample_size', 'avg_training_ridge', 'SD_train_ridge', 'CI_train_ridge', 'avg_test_ridge', 'SD_test_ridge', \n\u001b[0;32m      4\u001b[0m           \u001b[1;34m'CI_test_ridge'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'avg_training_lasso'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'SD_train_lasso'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'CI_train_lasso'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'avg_test_lasso'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'SD_test_lasso'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m           \u001b[1;34m'CI_test_lasso'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'avg_training_simpl'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'SD_train_simpl'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'CI_train_simpl'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'avg_test_simpl'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'SD_test_simpl'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'LinearRegression' is not defined"
     ]
    }
   ],
   "source": [
    "simpl_reg = LinearRegression()\n",
    "sample_sizes = [100, 150, 200, 250, 300, 350, 400]\n",
    "columns = ['sample_size', 'avg_training_ridge', 'SD_train_ridge', 'CI_train_ridge', 'avg_test_ridge', 'SD_test_ridge', \n",
    "          'CI_test_ridge', 'avg_training_lasso', 'SD_train_lasso', 'CI_train_lasso', 'avg_test_lasso', 'SD_test_lasso', \n",
    "          'CI_test_lasso', 'avg_training_simpl', 'SD_train_simpl', 'CI_train_simpl', 'avg_test_simpl', 'SD_test_simpl',\n",
    "          'CI_test_simpl']\n",
    "dicts = []\n",
    "\n",
    "# iterate through sample sizes\n",
    "for sample_size in sample_sizes:\n",
    "    \n",
    "    scores_training_ridge = []\n",
    "    scores_test_ridge = []\n",
    "    scores_training_lasso = []\n",
    "    scores_test_lasso = []\n",
    "    scores_training_simpl = []\n",
    "    scores_test_simpl = []\n",
    "    \n",
    "    # repeate 10 times for each fit\n",
    "    for i in range(0, 10):\n",
    "        sample_X, sample_y = sample(X_train, y_train, sample_size)\n",
    "        ridge_reg.fit(sample_X, sample_y)  \n",
    "        lasso_reg.fit(sample_X, sample_y)\n",
    "        simpl_reg.fit(sample_X, sample_y)\n",
    "        \n",
    "        pred_training_ridge = ridge_reg.predict(sample_X)\n",
    "        pred_test_ridge = ridge_reg.predict(X_test)\n",
    "        \n",
    "        pred_training_lasso = lasso_reg.predict(sample_X)\n",
    "        pred_test_lasso = lasso_reg.predict(X_test)\n",
    "        \n",
    "        pred_training_simpl =simpl_reg.predict(sample_X)\n",
    "        pred_test_simpl = simpl_reg.predict(X_test)\n",
    "        \n",
    "        # r2 scores\n",
    "        score_training_ridge = r2_score(sample_y, pred_training_ridge)\n",
    "        scores_training_ridge.append(score_training_ridge)\n",
    "        score_test_ridge = r2_score(y_test, pred_test_ridge)\n",
    "        scores_test_ridge.append(score_test_ridge)\n",
    "        \n",
    "        score_training_lasso = r2_score(sample_y, pred_training_lasso)\n",
    "        scores_training_lasso.append(score_training_lasso)\n",
    "        score_test_ridge = r2_score(y_test, pred_test_lasso)\n",
    "        scores_test_lasso.append(score_test_ridge)\n",
    "        \n",
    "        score_training_simpl = r2_score(sample_y, pred_training_simpl)\n",
    "        scores_training_simpl.append(score_training_simpl)\n",
    "        score_test_simpl = r2_score(y_test, pred_test_simpl)\n",
    "        scores_test_simpl.append(score_test_simpl)\n",
    "        \n",
    "    # averages over the 10 trials\n",
    "    avg_test_ridge = np.mean(scores_test_ridge)\n",
    "    avg_test_lasso = np.mean(scores_test_lasso)\n",
    "    avg_test_simpl = np.mean(scores_test_simpl)\n",
    "    avg_training_ridge = np.mean(scores_training_ridge)\n",
    "    avg_training_lasso = np.mean(scores_training_lasso)\n",
    "    avg_training_simpl = np.mean(scores_training_simpl)\n",
    "        \n",
    "    # compute standard deviations of errors for training and test sets for each regression\n",
    "    SD_train_ridge = np.std(scores_training_ridge)\n",
    "    SD_train_lasso = np.std(scores_training_lasso)\n",
    "    SD_train_simpl = np.std(scores_training_simpl)\n",
    "    SD_test_ridge = np.std(scores_test_ridge)\n",
    "    SD_test_lasso = np.std(scores_test_lasso)\n",
    "    SD_test_simpl = np.std(scores_test_simpl)\n",
    "    \n",
    "    # compute confidance intervals\n",
    "    CI_train_ridge = [avg_training_ridge-SD_train_ridge, avg_training_ridge+SD_train_ridge]\n",
    "    CI_train_lasso = [avg_training_lasso-SD_train_lasso, avg_training_lasso+SD_train_lasso]\n",
    "    CI_train_simpl = [avg_training_simpl-SD_train_simpl, avg_training_simpl+SD_train_simpl]\n",
    "    CI_test_ridge = [avg_test_ridge-SD_train_ridge, avg_test_ridge+SD_test_ridge]\n",
    "    CI_test_lasso = [avg_test_lasso-SD_train_lasso, avg_test_lasso+SD_test_lasso]\n",
    "    CI_test_simpl = [avg_test_simpl-SD_train_simpl, avg_test_simpl+SD_test_simpl]\n",
    "   \n",
    "   \n",
    "    \n",
    "    # create a data structure to store our values\n",
    "    temp_dict = {'sample_size': sample_size, 'avg_training_ridge' : avg_training_ridge, 'SD_train_ridge' : SD_train_ridge, 'CI_train_ridge' : CI_train_ridge, 'avg_test_ridge' : avg_test_ridge, \n",
    "                'CI_test_simpl' : CI_test_simpl, 'CI_test_lasso' : CI_test_lasso, 'CI_test_ridge' : CI_test_ridge, 'CI_train_simpl' : CI_train_simpl, 'CI_train_lasso' :  CI_train_lasso, 'SD_test_lasso' : SD_test_lasso, 'SD_test_ridge' :  SD_test_ridge, 'avg_training_lasso' : avg_training_lasso, 'SD_train_lasso' : SD_train_lasso, 'avg_test_lasso' : avg_test_lasso,\n",
    "                'avg_training_simpl' : avg_training_simpl, 'SD_train_simpl' : SD_train_simpl, 'avg_test_simpl' : avg_test_simpl, 'SD_test_simpl' : SD_test_simpl} \n",
    "    dicts.append(temp_dict)\n",
    "   \n",
    "df = pd.DataFrame(dicts) \n",
    "df = df[columns]\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample_size</th>\n",
       "      <th>avg_training_ridge</th>\n",
       "      <th>SD_train_ridge</th>\n",
       "      <th>CI_train_ridge</th>\n",
       "      <th>avg_test_ridge</th>\n",
       "      <th>SD_test_ridge</th>\n",
       "      <th>CI_test_ridge</th>\n",
       "      <th>avg_training_lasso</th>\n",
       "      <th>SD_train_lasso</th>\n",
       "      <th>CI_train_lasso</th>\n",
       "      <th>avg_test_lasso</th>\n",
       "      <th>SD_test_lasso</th>\n",
       "      <th>CI_test_lasso</th>\n",
       "      <th>avg_training_simpl</th>\n",
       "      <th>SD_train_simpl</th>\n",
       "      <th>CI_train_simpl</th>\n",
       "      <th>avg_test_simpl</th>\n",
       "      <th>SD_test_simpl</th>\n",
       "      <th>CI_test_simpl</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100</td>\n",
       "      <td>0.584111</td>\n",
       "      <td>0.061805</td>\n",
       "      <td>[0.522305720356, 0.645915672641]</td>\n",
       "      <td>0.228526</td>\n",
       "      <td>0.020185</td>\n",
       "      <td>[0.166721249746, 0.248711377621]</td>\n",
       "      <td>0.658240</td>\n",
       "      <td>0.058756</td>\n",
       "      <td>[0.599484737069, 0.716996208018]</td>\n",
       "      <td>0.168355</td>\n",
       "      <td>0.033415</td>\n",
       "      <td>[0.10959922494, 0.201770134448]</td>\n",
       "      <td>0.685695</td>\n",
       "      <td>0.056708</td>\n",
       "      <td>[0.628987523738, 0.742403107772]</td>\n",
       "      <td>0.059502</td>\n",
       "      <td>0.061097</td>\n",
       "      <td>[0.00279422681035, 0.120599109564]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>150</td>\n",
       "      <td>0.592100</td>\n",
       "      <td>0.055601</td>\n",
       "      <td>[0.536498991647, 0.647700958723]</td>\n",
       "      <td>0.237000</td>\n",
       "      <td>0.020412</td>\n",
       "      <td>[0.181399237581, 0.257412376422]</td>\n",
       "      <td>0.628095</td>\n",
       "      <td>0.056481</td>\n",
       "      <td>[0.571613605175, 0.684575706423]</td>\n",
       "      <td>0.201860</td>\n",
       "      <td>0.042777</td>\n",
       "      <td>[0.145379303288, 0.244637721039]</td>\n",
       "      <td>0.646911</td>\n",
       "      <td>0.055654</td>\n",
       "      <td>[0.59125696497, 0.70256543492]</td>\n",
       "      <td>0.113155</td>\n",
       "      <td>0.086066</td>\n",
       "      <td>[0.0575006405177, 0.19922060836]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>200</td>\n",
       "      <td>0.590108</td>\n",
       "      <td>0.040498</td>\n",
       "      <td>[0.549610243525, 0.630606648324]</td>\n",
       "      <td>0.240662</td>\n",
       "      <td>0.015697</td>\n",
       "      <td>[0.200164215977, 0.256359660889]</td>\n",
       "      <td>0.615130</td>\n",
       "      <td>0.042261</td>\n",
       "      <td>[0.572869636809, 0.657390923382]</td>\n",
       "      <td>0.231127</td>\n",
       "      <td>0.021269</td>\n",
       "      <td>[0.188866347552, 0.252395990457]</td>\n",
       "      <td>0.633282</td>\n",
       "      <td>0.040676</td>\n",
       "      <td>[0.59260619523, 0.673957480372]</td>\n",
       "      <td>0.184191</td>\n",
       "      <td>0.040090</td>\n",
       "      <td>[0.143515601024, 0.224281232824]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>250</td>\n",
       "      <td>0.591408</td>\n",
       "      <td>0.045712</td>\n",
       "      <td>[0.545695866803, 0.637120622897]</td>\n",
       "      <td>0.245105</td>\n",
       "      <td>0.016984</td>\n",
       "      <td>[0.199392577533, 0.26208867026]</td>\n",
       "      <td>0.608617</td>\n",
       "      <td>0.044767</td>\n",
       "      <td>[0.563850172043, 0.653384234213]</td>\n",
       "      <td>0.235626</td>\n",
       "      <td>0.018888</td>\n",
       "      <td>[0.19085901504, 0.254513914341]</td>\n",
       "      <td>0.625950</td>\n",
       "      <td>0.043840</td>\n",
       "      <td>[0.582109731696, 0.669790429708]</td>\n",
       "      <td>0.189129</td>\n",
       "      <td>0.022281</td>\n",
       "      <td>[0.145288359709, 0.211409420043]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>300</td>\n",
       "      <td>0.587758</td>\n",
       "      <td>0.032593</td>\n",
       "      <td>[0.555164295128, 0.62035076596]</td>\n",
       "      <td>0.239252</td>\n",
       "      <td>0.014442</td>\n",
       "      <td>[0.206658745073, 0.253693502282]</td>\n",
       "      <td>0.598486</td>\n",
       "      <td>0.031241</td>\n",
       "      <td>[0.567245027106, 0.629726456089]</td>\n",
       "      <td>0.233421</td>\n",
       "      <td>0.019176</td>\n",
       "      <td>[0.202180664517, 0.252596918123]</td>\n",
       "      <td>0.612080</td>\n",
       "      <td>0.031616</td>\n",
       "      <td>[0.580464368149, 0.643696253099]</td>\n",
       "      <td>0.198303</td>\n",
       "      <td>0.022502</td>\n",
       "      <td>[0.166686858086, 0.220804732959]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>350</td>\n",
       "      <td>0.600972</td>\n",
       "      <td>0.022318</td>\n",
       "      <td>[0.578653757389, 0.623289840309]</td>\n",
       "      <td>0.238852</td>\n",
       "      <td>0.015983</td>\n",
       "      <td>[0.216534400969, 0.254835411852]</td>\n",
       "      <td>0.607589</td>\n",
       "      <td>0.022950</td>\n",
       "      <td>[0.584639498744, 0.630539113617]</td>\n",
       "      <td>0.235696</td>\n",
       "      <td>0.021157</td>\n",
       "      <td>[0.212746318632, 0.256853357648]</td>\n",
       "      <td>0.622423</td>\n",
       "      <td>0.022351</td>\n",
       "      <td>[0.600072288636, 0.64477364528]</td>\n",
       "      <td>0.198688</td>\n",
       "      <td>0.047105</td>\n",
       "      <td>[0.176337162978, 0.245792547601]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>400</td>\n",
       "      <td>0.576583</td>\n",
       "      <td>0.037033</td>\n",
       "      <td>[0.539550397854, 0.613615565531]</td>\n",
       "      <td>0.246311</td>\n",
       "      <td>0.014794</td>\n",
       "      <td>[0.209277917627, 0.261104559005]</td>\n",
       "      <td>0.581022</td>\n",
       "      <td>0.038925</td>\n",
       "      <td>[0.542096389943, 0.619946657636]</td>\n",
       "      <td>0.249951</td>\n",
       "      <td>0.021360</td>\n",
       "      <td>[0.211025544128, 0.271310484174]</td>\n",
       "      <td>0.593761</td>\n",
       "      <td>0.040855</td>\n",
       "      <td>[0.552906541443, 0.634615574755]</td>\n",
       "      <td>0.221268</td>\n",
       "      <td>0.036158</td>\n",
       "      <td>[0.180413796837, 0.257426564103]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sample_size  avg_training_ridge  SD_train_ridge  \\\n",
       "0          100            0.584111        0.061805   \n",
       "1          150            0.592100        0.055601   \n",
       "2          200            0.590108        0.040498   \n",
       "3          250            0.591408        0.045712   \n",
       "4          300            0.587758        0.032593   \n",
       "5          350            0.600972        0.022318   \n",
       "6          400            0.576583        0.037033   \n",
       "\n",
       "                     CI_train_ridge  avg_test_ridge  SD_test_ridge  \\\n",
       "0  [0.522305720356, 0.645915672641]        0.228526       0.020185   \n",
       "1  [0.536498991647, 0.647700958723]        0.237000       0.020412   \n",
       "2  [0.549610243525, 0.630606648324]        0.240662       0.015697   \n",
       "3  [0.545695866803, 0.637120622897]        0.245105       0.016984   \n",
       "4   [0.555164295128, 0.62035076596]        0.239252       0.014442   \n",
       "5  [0.578653757389, 0.623289840309]        0.238852       0.015983   \n",
       "6  [0.539550397854, 0.613615565531]        0.246311       0.014794   \n",
       "\n",
       "                      CI_test_ridge  avg_training_lasso  SD_train_lasso  \\\n",
       "0  [0.166721249746, 0.248711377621]            0.658240        0.058756   \n",
       "1  [0.181399237581, 0.257412376422]            0.628095        0.056481   \n",
       "2  [0.200164215977, 0.256359660889]            0.615130        0.042261   \n",
       "3   [0.199392577533, 0.26208867026]            0.608617        0.044767   \n",
       "4  [0.206658745073, 0.253693502282]            0.598486        0.031241   \n",
       "5  [0.216534400969, 0.254835411852]            0.607589        0.022950   \n",
       "6  [0.209277917627, 0.261104559005]            0.581022        0.038925   \n",
       "\n",
       "                     CI_train_lasso  avg_test_lasso  SD_test_lasso  \\\n",
       "0  [0.599484737069, 0.716996208018]        0.168355       0.033415   \n",
       "1  [0.571613605175, 0.684575706423]        0.201860       0.042777   \n",
       "2  [0.572869636809, 0.657390923382]        0.231127       0.021269   \n",
       "3  [0.563850172043, 0.653384234213]        0.235626       0.018888   \n",
       "4  [0.567245027106, 0.629726456089]        0.233421       0.019176   \n",
       "5  [0.584639498744, 0.630539113617]        0.235696       0.021157   \n",
       "6  [0.542096389943, 0.619946657636]        0.249951       0.021360   \n",
       "\n",
       "                      CI_test_lasso  avg_training_simpl  SD_train_simpl  \\\n",
       "0   [0.10959922494, 0.201770134448]            0.685695        0.056708   \n",
       "1  [0.145379303288, 0.244637721039]            0.646911        0.055654   \n",
       "2  [0.188866347552, 0.252395990457]            0.633282        0.040676   \n",
       "3   [0.19085901504, 0.254513914341]            0.625950        0.043840   \n",
       "4  [0.202180664517, 0.252596918123]            0.612080        0.031616   \n",
       "5  [0.212746318632, 0.256853357648]            0.622423        0.022351   \n",
       "6  [0.211025544128, 0.271310484174]            0.593761        0.040855   \n",
       "\n",
       "                     CI_train_simpl  avg_test_simpl  SD_test_simpl  \\\n",
       "0  [0.628987523738, 0.742403107772]        0.059502       0.061097   \n",
       "1    [0.59125696497, 0.70256543492]        0.113155       0.086066   \n",
       "2   [0.59260619523, 0.673957480372]        0.184191       0.040090   \n",
       "3  [0.582109731696, 0.669790429708]        0.189129       0.022281   \n",
       "4  [0.580464368149, 0.643696253099]        0.198303       0.022502   \n",
       "5   [0.600072288636, 0.64477364528]        0.198688       0.047105   \n",
       "6  [0.552906541443, 0.634615574755]        0.221268       0.036158   \n",
       "\n",
       "                        CI_test_simpl  \n",
       "0  [0.00279422681035, 0.120599109564]  \n",
       "1    [0.0575006405177, 0.19922060836]  \n",
       "2    [0.143515601024, 0.224281232824]  \n",
       "3    [0.145288359709, 0.211409420043]  \n",
       "4    [0.166686858086, 0.220804732959]  \n",
       "5    [0.176337162978, 0.245792547601]  \n",
       "6    [0.180413796837, 0.257426564103]  "
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA3QAAAF1CAYAAACgU6g3AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJEmS1FImdJIkSZLUUv8f7tx0Zl9JVcAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f9dbd332a20>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA3QAAAF1CAYAAACgU6g3AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f9dbc53c550>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "f, (ax1, ax2, ax3) = plt.subplots(1, 3, sharey=True, figsize=(15,6))\n",
    "\n",
    "\n",
    "ax1.errorbar(df.sample_size.values, df.avg_training_ridge.values, c='red', yerr=df.SD_train_ridge.values, fmt='o')\n",
    "ax1.set_title(\"Ridge Training R2 Scores\")\n",
    "ax1.set_ylabel=\"R2\"\n",
    "ax1.set_xlabel=\"sample size\"\n",
    "\n",
    "ax2.errorbar(df.sample_size.values, df.avg_training_lasso.values, c='red', yerr=df.SD_train_lasso.values, fmt='o')\n",
    "ax2.set_title(\"Lasso Training R2 Scores\")\n",
    "\n",
    "ax3.errorbar(df.sample_size.values, df.avg_training_simpl.values, c='red', yerr=df.SD_train_simpl.values, fmt='o')\n",
    "ax3.set_title(\"Simple Regression Training R2 Scores\")\n",
    "\n",
    "f, (ax1, ax2, ax3) = plt.subplots(1, 3, sharey=True, figsize=(15,6) )\n",
    "ax1.errorbar(df.sample_size.values, df.avg_test_ridge.values, c='red', yerr=df.SD_test_ridge.values, fmt='o')\n",
    "ax1.set_title(\"Ridge Test R2 Scores\")\n",
    "\n",
    "ax2.errorbar(df.sample_size.values, df.avg_test_lasso.values, c='red', yerr=df.SD_test_lasso.values, fmt='o')\n",
    "ax2.set_title(\"Lasso Test R2 Scores\")\n",
    "\n",
    "ax3.errorbar(df.sample_size.values, df.avg_test_simpl.values, c='red', yerr=df.SD_test_simpl.values, fmt='o')\n",
    "ax3.set_title(\"Simple Regression Test R2 Scores\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Fit linear, Ridge and Lasso regression models to each of the generated sample. In each case, compute the $R^2$ score for the model on the training sample on which it was fitted, and on the test set.\n",
    "- Repeat the above experiment for 10 random trials/splits, and compute the average train and test $R^2$ across the trials for each training sample size. Also, compute the standard deviation (SD) in each case.\n",
    "- Make a plot of the mean training $R^2$ scores for the linear, Ridge and Lasso regression methods as a function of the training sample size. Also, show a confidence interval for the mean scores extending from **mean - SD** to **mean + SD**. Make a similar plot for the test $R^2$ scores.\n",
    "\n",
    "How do the training and test $R^2$ scores compare for the three methods? Give an explanation for your observations. How do the confidence intervals for the estimated $R^2$ change with training sample size? Based on the plots, which of the three methods would you recommend when one needs to fit a regression model using a small training sample?\n",
    "\n",
    "*Hint:* You may use `sklearn`'s `RidgeCV` and `LassoCV` classes to implement Ridge and Lasso regression. These classes automatically perform cross-validation to tune the parameter $\\lambda$ from a given range of values. You may use the `plt.errorbar` function to plot confidence bars for the average $R^2$ scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Analysis*\n",
    "\n",
    "There is an inverse relationship between test and training R2 for the 3 regressions. The simple regression has the highest training R2. This is to be expected, as there is no penalty. However, the test R2 suffers as a result of this overfitting. The Lasso and Ridge, while having lower training R2, have much better performance on the testing data as a result of the regularization penalty. Ridge performs the best of the three for predicting on the test set for all sample sizes, Lasso only performs as well as Ridge when the sample size is large. As far as the confidence intervals, they tighten as the sample size increases. This is to be expected. Given a small sample size, we would use Ridge regression with an alpha value of 10."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part (g): Polynomial & Interaction Terms\n",
    "\n",
    "Moving beyond linear models, we will now try to improve the performance of the regression model in Part (b) from HW 3 by including higher-order polynomial and interaction terms. \n",
    "\n",
    "- For each continuous predictor $X_j$, include additional polynomial terms $X^2_j$, $X^3_j$, and $X^4_j$, and fit a multiple regression model to the expanded training set. How does the $R^2$ of this model on the test set compare with that of the linear model fitted in Part (b) from HW 3? Using a t-test, find out which of estimated coefficients for the polynomial terms are statistically significant at a significance level of 5%. \n",
    "\n",
    "- Fit a multiple linear regression model with additional interaction terms $\\mathbb{I}_{month = 12} \\times temp$ and $\\mathbb{I}_{workingday = 1} \\times \\mathbb{I}_{weathersit = 1}$ and report the test $R^2$ for the fitted model. How does this compare with the $R^2$ obtained using linear model in Part (b) from HW 3? Are the estimated coefficients for the interaction terms statistically significant at a significance level of 5%?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Analysis*\n",
    "\n",
    "Our R2 on this part, compared to homework three is significantly better. Of our 40 predictors, 9 seem statistically significant based upon p values obtained from OLS. A few others are very close, perhaps warranting inclusion. Our model still has high dimensionality and high collinaearity, however."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data['temp^2'] = train_data['temp']**2\n",
    "test_data['temp^2'] = test_data['temp']**2\n",
    "train_data['temp^3'] = train_data['temp']**3\n",
    "test_data['temp^3'] = test_data['temp']**3\n",
    "train_data['temp^4'] = train_data['temp']**4\n",
    "test_data['temp^4'] = test_data['temp']**4\n",
    "\n",
    "train_data['atemp^2'] = (train_data['atemp'])**2\n",
    "test_data['atemp^2'] = (test_data['atemp'])**2\n",
    "train_data['atemp^3'] = (train_data['atemp'])**3\n",
    "test_data['atemp^3'] = (test_data['atemp'])**3\n",
    "train_data['atemp^4'] = (train_data['atemp'])**4\n",
    "test_data['atemp^4'] = (test_data['atemp'])**4\n",
    "\n",
    "train_data['humidity^2'] = (train_data['humidity'])**2\n",
    "test_data['humidity^2'] = (test_data['humidity'])**2\n",
    "train_data['humidity^3'] = (train_data['humidity'])**3\n",
    "test_data['humidity^3'] = (test_data['humidity'])**3\n",
    "train_data['humidity^4'] = (train_data['humidity'])**4\n",
    "test_data['humidity^4'] = (test_data['humidity'])**4\n",
    "\n",
    "train_data['windspeed^2'] = (train_data['windspeed'])**2\n",
    "test_data['windspeed^2'] = (test_data['windspeed'])**2\n",
    "train_data['windspeed^3'] = (train_data['windspeed'])**3\n",
    "test_data['windspeed^3'] = (test_data['windspeed'])**3\n",
    "train_data['windspeed^4'] = (train_data['windspeed'])**4\n",
    "test_data['windspeed^4'] = (test_data['windspeed'])**4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(331, 40) (331, 1) (400, 40) (400, 1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>holiday</th>\n",
       "      <th>workingday</th>\n",
       "      <th>temp</th>\n",
       "      <th>atemp</th>\n",
       "      <th>humidity</th>\n",
       "      <th>windspeed</th>\n",
       "      <th>rentals</th>\n",
       "      <th>season_2</th>\n",
       "      <th>season_3</th>\n",
       "      <th>season_4</th>\n",
       "      <th>month_2</th>\n",
       "      <th>month_3</th>\n",
       "      <th>month_4</th>\n",
       "      <th>month_5</th>\n",
       "      <th>month_6</th>\n",
       "      <th>month_7</th>\n",
       "      <th>month_8</th>\n",
       "      <th>month_9</th>\n",
       "      <th>month_10</th>\n",
       "      <th>month_11</th>\n",
       "      <th>month_12</th>\n",
       "      <th>day_of_week_1</th>\n",
       "      <th>day_of_week_2</th>\n",
       "      <th>day_of_week_3</th>\n",
       "      <th>day_of_week_4</th>\n",
       "      <th>day_of_week_5</th>\n",
       "      <th>day_of_week_6</th>\n",
       "      <th>weather_2</th>\n",
       "      <th>weather_3</th>\n",
       "      <th>temp^2</th>\n",
       "      <th>temp^3</th>\n",
       "      <th>temp^4</th>\n",
       "      <th>atemp^2</th>\n",
       "      <th>atemp^3</th>\n",
       "      <th>atemp^4</th>\n",
       "      <th>humidity^2</th>\n",
       "      <th>humidity^3</th>\n",
       "      <th>humidity^4</th>\n",
       "      <th>windspeed^2</th>\n",
       "      <th>windspeed^3</th>\n",
       "      <th>windspeed^4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.623798</td>\n",
       "      <td>0.650106</td>\n",
       "      <td>0.920664</td>\n",
       "      <td>-0.928758</td>\n",
       "      <td>6073.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.389124</td>\n",
       "      <td>0.242735</td>\n",
       "      <td>0.151418</td>\n",
       "      <td>0.422637</td>\n",
       "      <td>0.274759</td>\n",
       "      <td>0.178622</td>\n",
       "      <td>0.847622</td>\n",
       "      <td>0.780375</td>\n",
       "      <td>0.718464</td>\n",
       "      <td>0.862591</td>\n",
       "      <td>-0.801139</td>\n",
       "      <td>0.744064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.180310</td>\n",
       "      <td>-0.054759</td>\n",
       "      <td>0.696852</td>\n",
       "      <td>-0.213502</td>\n",
       "      <td>6606.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.032512</td>\n",
       "      <td>-0.005862</td>\n",
       "      <td>0.001057</td>\n",
       "      <td>0.002998</td>\n",
       "      <td>-0.000164</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.485603</td>\n",
       "      <td>0.338393</td>\n",
       "      <td>0.235810</td>\n",
       "      <td>0.045583</td>\n",
       "      <td>-0.009732</td>\n",
       "      <td>0.002078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.802489</td>\n",
       "      <td>0.851495</td>\n",
       "      <td>-0.448383</td>\n",
       "      <td>0.803926</td>\n",
       "      <td>7363.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.643989</td>\n",
       "      <td>0.516794</td>\n",
       "      <td>0.414722</td>\n",
       "      <td>0.725044</td>\n",
       "      <td>0.617372</td>\n",
       "      <td>0.525689</td>\n",
       "      <td>0.201047</td>\n",
       "      <td>-0.090146</td>\n",
       "      <td>0.040420</td>\n",
       "      <td>0.646297</td>\n",
       "      <td>0.519575</td>\n",
       "      <td>0.417699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.520492</td>\n",
       "      <td>-1.565182</td>\n",
       "      <td>-0.332113</td>\n",
       "      <td>-0.269099</td>\n",
       "      <td>2431.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.311895</td>\n",
       "      <td>-3.515217</td>\n",
       "      <td>5.344859</td>\n",
       "      <td>2.449794</td>\n",
       "      <td>-3.834373</td>\n",
       "      <td>6.001490</td>\n",
       "      <td>0.110299</td>\n",
       "      <td>-0.036632</td>\n",
       "      <td>0.012166</td>\n",
       "      <td>0.072414</td>\n",
       "      <td>-0.019487</td>\n",
       "      <td>0.005244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.534453</td>\n",
       "      <td>0.348021</td>\n",
       "      <td>1.975789</td>\n",
       "      <td>-1.199027</td>\n",
       "      <td>1996.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.285640</td>\n",
       "      <td>0.152661</td>\n",
       "      <td>0.081590</td>\n",
       "      <td>0.121119</td>\n",
       "      <td>0.042152</td>\n",
       "      <td>0.014670</td>\n",
       "      <td>3.903744</td>\n",
       "      <td>7.712976</td>\n",
       "      <td>15.239217</td>\n",
       "      <td>1.437666</td>\n",
       "      <td>-1.723801</td>\n",
       "      <td>2.066885</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   holiday  workingday      temp     atemp  humidity  windspeed  rentals  \\\n",
       "0      0.0         1.0  0.623798  0.650106  0.920664  -0.928758   6073.0   \n",
       "1      0.0         1.0 -0.180310 -0.054759  0.696852  -0.213502   6606.0   \n",
       "2      0.0         1.0  0.802489  0.851495 -0.448383   0.803926   7363.0   \n",
       "3      0.0         0.0 -1.520492 -1.565182 -0.332113  -0.269099   2431.0   \n",
       "4      0.0         1.0  0.534453  0.348021  1.975789  -1.199027   1996.0   \n",
       "\n",
       "   season_2  season_3  season_4  month_2  month_3  month_4  month_5  month_6  \\\n",
       "0         1         0         0        0        0        0        1        0   \n",
       "1         0         0         1        0        0        0        0        0   \n",
       "2         1         0         0        0        0        0        0        1   \n",
       "3         0         0         1        0        0        0        0        0   \n",
       "4         0         1         0        0        0        0        0        0   \n",
       "\n",
       "   month_7  month_8  month_9  month_10  month_11  month_12  day_of_week_1  \\\n",
       "0        0        0        0         0         0         0              0   \n",
       "1        0        0        0         0         0         1              0   \n",
       "2        0        0        0         0         0         0              0   \n",
       "3        0        0        0         0         0         1              0   \n",
       "4        0        0        1         0         0         0              0   \n",
       "\n",
       "   day_of_week_2  day_of_week_3  day_of_week_4  day_of_week_5  day_of_week_6  \\\n",
       "0              1              0              0              0              0   \n",
       "1              1              0              0              0              0   \n",
       "2              0              0              1              0              0   \n",
       "3              0              0              0              0              0   \n",
       "4              0              1              0              0              0   \n",
       "\n",
       "   weather_2  weather_3    temp^2    temp^3    temp^4   atemp^2   atemp^3  \\\n",
       "0          1          0  0.389124  0.242735  0.151418  0.422637  0.274759   \n",
       "1          0          0  0.032512 -0.005862  0.001057  0.002998 -0.000164   \n",
       "2          0          0  0.643989  0.516794  0.414722  0.725044  0.617372   \n",
       "3          0          0  2.311895 -3.515217  5.344859  2.449794 -3.834373   \n",
       "4          0          1  0.285640  0.152661  0.081590  0.121119  0.042152   \n",
       "\n",
       "    atemp^4  humidity^2  humidity^3  humidity^4  windspeed^2  windspeed^3  \\\n",
       "0  0.178622    0.847622    0.780375    0.718464     0.862591    -0.801139   \n",
       "1  0.000009    0.485603    0.338393    0.235810     0.045583    -0.009732   \n",
       "2  0.525689    0.201047   -0.090146    0.040420     0.646297     0.519575   \n",
       "3  6.001490    0.110299   -0.036632    0.012166     0.072414    -0.019487   \n",
       "4  0.014670    3.903744    7.712976   15.239217     1.437666    -1.723801   \n",
       "\n",
       "   windspeed^4  \n",
       "0     0.744064  \n",
       "1     0.002078  \n",
       "2     0.417699  \n",
       "3     0.005244  \n",
       "4     2.066885  "
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create an array of values for our regression\n",
    "y_train = train_data['rentals'].values\n",
    "X_train = train_data[['holiday', 'workingday', 'temp', 'atemp', 'humidity', 'windspeed', 'season_2', \n",
    "                 'season_3', 'season_4', 'month_2', 'month_3', 'month_4', 'month_5', 'month_6',\n",
    "                 'month_7', 'month_8', 'month_9', 'month_10', 'month_11', 'month_12', 'day_of_week_1', \n",
    "                 'day_of_week_2', 'day_of_week_3', 'day_of_week_4', 'day_of_week_5', 'day_of_week_6', 'weather_2', \n",
    "                 'weather_3', 'temp^2', 'temp^3', 'temp^4', 'atemp^2', 'atemp^3', 'atemp^4', 'humidity^2', 'humidity^3', \n",
    "                 'humidity^4', 'windspeed^2', 'windspeed^3', 'windspeed^4']].values\n",
    "\n",
    "y_test = test_data['rentals'].values\n",
    "X_test = test_data[['holiday', 'workingday', 'temp', 'atemp', 'humidity', 'windspeed', 'season_2', \n",
    "                 'season_3', 'season_4', 'month_2', 'month_3', 'month_4', 'month_5', 'month_6',\n",
    "                 'month_7', 'month_8', 'month_9', 'month_10', 'month_11', 'month_12', 'day_of_week_1', \n",
    "                 'day_of_week_2', 'day_of_week_3', 'day_of_week_4', 'day_of_week_5', 'day_of_week_6', 'weather_2', \n",
    "                 'weather_3', 'temp^2', 'temp^3', 'temp^4', 'atemp^2', 'atemp^3', 'atemp^4', 'humidity^2', 'humidity^3', \n",
    "                 'humidity^4', 'windspeed^2', 'windspeed^3', 'windspeed^4']].values\n",
    "\n",
    "y_train = y_train.reshape(len(y_train), 1)\n",
    "y_test = y_test.reshape(len(y_test), 1)\n",
    "\n",
    "print(X_train.shape, y_train.shape, X_test.shape , y_test.shape)\n",
    "\n",
    "\n",
    "train_data.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The equation of the regression plane is: [ 5035.27125887] + [[ -189.7675006    351.27394405   771.48662326   897.2756023   -668.91446319\n",
      "   -446.50850455   766.43070371  1578.75436674  1523.2288234   -325.06857397\n",
      "   -304.84911267  -418.02446177 -1037.20424547 -1456.18565573\n",
      "  -1416.98816779 -1715.93889094 -1073.40080145  -925.87103867\n",
      "   -825.53284158  -555.66756465   -93.32647706  -133.42791146   147.7312741\n",
      "     30.59243395   209.93712392   471.0834343     59.01188326\n",
      "  -1043.99967412 -1811.01797233     8.60775572   -45.191024    1175.50049569\n",
      "   -303.93578541   -20.76855912   -53.67085686   -16.05763165   -24.8367323\n",
      "    -34.16534669    44.8338792    -20.17694855]] * x\n",
      "The train MSE is 1233551.6134555764, the test MSE is 3157061.4234926915\n",
      "The train R^2 is 0.6696562402214016, the test R^2 is 0.27723843508615387\n"
     ]
    }
   ],
   "source": [
    "# fit model with the additional interaction terms, compute metrics\n",
    "lm = LinearRegression(fit_intercept=True)\n",
    "lm.fit(X_train, y_train)\n",
    "\n",
    "y_pred = lm.predict(X_test)\n",
    "\n",
    "print('The equation of the regression plane is: {} + {} * x'.format(lm.intercept_, lm.coef_))\n",
    "\n",
    "train_MSE= np.mean((y_train - lm.predict(X_train))**2)\n",
    "test_MSE= np.mean((y_test - lm.predict(X_test))**2)\n",
    "print('The train MSE is {}, the test MSE is {}'.format(train_MSE, test_MSE))\n",
    "\n",
    "train_R_sq = lm.score(X_train, y_train)\n",
    "test_R_sq = lm.score(X_test, y_test)\n",
    "print('The train R^2 is {}, the test R^2 is {}'.format(train_R_sq, test_R_sq))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   R-squared:                       0.670\n",
      "Model:                            OLS   Adj. R-squared:                  0.625\n",
      "Method:                 Least Squares   F-statistic:                     15.13\n",
      "Date:                Thu, 12 Oct 2017   Prob (F-statistic):           7.98e-50\n",
      "Time:                        02:47:40   Log-Likelihood:                -2790.9\n",
      "No. Observations:                 331   AIC:                             5662.\n",
      "Df Residuals:                     291   BIC:                             5814.\n",
      "Df Model:                          39                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const       5035.2713    460.417     10.936      0.000    4129.101    5941.442\n",
      "x1          -189.7675    365.157     -0.520      0.604    -908.451     528.916\n",
      "x2           351.2739    150.615      2.332      0.020      54.841     647.707\n",
      "x3           771.4866    760.117      1.015      0.311    -724.536    2267.510\n",
      "x4           897.2756    713.172      1.258      0.209    -506.353    2300.904\n",
      "x5          -668.9145    157.356     -4.251      0.000    -978.615    -359.214\n",
      "x6          -446.5085    148.929     -2.998      0.003    -739.623    -153.394\n",
      "x7           766.4307    454.546      1.686      0.093    -128.185    1661.046\n",
      "x8          1578.7544    519.364      3.040      0.003     556.569    2600.940\n",
      "x9          1523.2288    467.580      3.258      0.001     602.961    2443.496\n",
      "x10         -325.0686    409.611     -0.794      0.428   -1131.245     481.108\n",
      "x11         -304.8491    446.028     -0.683      0.495   -1182.700     573.002\n",
      "x12         -418.0245    639.524     -0.654      0.514   -1676.703     840.654\n",
      "x13        -1037.2042    677.186     -1.532      0.127   -2370.008     295.599\n",
      "x14        -1456.1857    697.520     -2.088      0.038   -2829.010     -83.362\n",
      "x15        -1416.9882    749.751     -1.890      0.060   -2892.610      58.634\n",
      "x16        -1715.9389    743.240     -2.309      0.022   -3178.747    -253.131\n",
      "x17        -1073.4008    660.859     -1.624      0.105   -2374.069     227.268\n",
      "x18         -925.8710    617.522     -1.499      0.135   -2141.247     289.505\n",
      "x19         -825.5328    591.138     -1.397      0.164   -1988.981     337.916\n",
      "x20         -555.6676    479.543     -1.159      0.248   -1499.481     388.146\n",
      "x21          -93.3265    156.015     -0.598      0.550    -400.387     213.734\n",
      "x22         -133.4279    184.734     -0.722      0.471    -497.012     230.156\n",
      "x23          147.7313    195.071      0.757      0.449    -236.197     531.660\n",
      "x24           30.5924    187.547      0.163      0.871    -338.528     399.713\n",
      "x25          209.9371    182.024      1.153      0.250    -148.313     568.187\n",
      "x26          471.0834    246.557      1.911      0.057     -14.178     956.345\n",
      "x27           59.0119    196.208      0.301      0.764    -327.155     445.179\n",
      "x28        -1043.9997    546.051     -1.912      0.057   -2118.709      30.710\n",
      "x29        -1811.0180    816.910     -2.217      0.027   -3418.820    -203.216\n",
      "x30            8.6078    275.731      0.031      0.975    -534.071     551.287\n",
      "x31          -45.1910    171.419     -0.264      0.792    -382.570     292.188\n",
      "x32         1175.5005    788.864      1.490      0.137    -377.102    2728.103\n",
      "x33         -303.9358    246.097     -1.235      0.218    -788.292     180.420\n",
      "x34          -20.7686    147.605     -0.141      0.888    -311.276     269.739\n",
      "x35          -53.6709    155.383     -0.345      0.730    -359.488     252.146\n",
      "x36          -16.0576     44.892     -0.358      0.721    -104.412      72.297\n",
      "x37          -24.8367     31.481     -0.789      0.431     -86.796      37.122\n",
      "x38          -34.1653    126.952     -0.269      0.788    -284.026     215.695\n",
      "x39           44.8339     65.459      0.685      0.494     -83.999     173.667\n",
      "x40          -20.1769     30.327     -0.665      0.506     -79.864      39.510\n",
      "==============================================================================\n",
      "Omnibus:                       29.995   Durbin-Watson:                   1.959\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):               10.202\n",
      "Skew:                          -0.094   Prob(JB):                      0.00609\n",
      "Kurtosis:                       2.161   Cond. No.                     1.35e+16\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The smallest eigenvalue is 2.1e-28. This might indicate that there are\n",
      "strong multicollinearity problems or that the design matrix is singular.\n",
      "Parameters:  [ 5035.27125887  -189.7675006    351.27394405   771.48662326   897.2756023\n",
      "  -668.91446319  -446.50850455   766.43070371  1578.75436674  1523.2288234\n",
      "  -325.06857397  -304.84911267  -418.02446177 -1037.20424547 -1456.18565573\n",
      " -1416.98816779 -1715.93889094 -1073.40080145  -925.87103867  -825.53284158\n",
      "  -555.66756465   -93.32647706  -133.42791146   147.7312741     30.59243395\n",
      "   209.93712392   471.0834343     59.01188326 -1043.99967412 -1811.01797233\n",
      "     8.60775572   -45.191024    1175.50049569  -303.93578541   -20.76855912\n",
      "   -53.67085686   -16.05763165   -24.8367323    -34.16534669    44.8338792\n",
      "   -20.17694855]\n"
     ]
    }
   ],
   "source": [
    "# statsmodel regression to easily obtain metrics\n",
    "\n",
    "# create the X matrix by appending a column of ones to x_train\n",
    "X = sm.add_constant(X_train)\n",
    "X_test_sm = sm.add_constant(X_test)\n",
    "# build the OLS model from the training data\n",
    "smm = sm.OLS(y_train, X)\n",
    "\n",
    "#save regression info in results_sm\n",
    "results_sm = smm.fit()\n",
    "\n",
    "print(results_sm.summary())\n",
    "print('Parameters: ', results_sm.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data['month12_temp'] = train_data['month_12'] * train_data['temp']\n",
    "\n",
    "train_data['workday_weather'] = np.where((train_data['workingday'] ==1) & (train_data['weather_2'] ==0) & \n",
    "                                (train_data['weather_3']==0), 1, 0)\n",
    "\n",
    "test_data['month12_temp'] = test_data['month_12'] * test_data['temp']\n",
    "\n",
    "test_data['workday_weather'] = np.where((test_data['workingday'] ==1) & (test_data['weather_2'] ==0) & \n",
    "                                (test_data['weather_3']==0), 1, 0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>holiday</th>\n",
       "      <th>workingday</th>\n",
       "      <th>temp</th>\n",
       "      <th>atemp</th>\n",
       "      <th>humidity</th>\n",
       "      <th>windspeed</th>\n",
       "      <th>rentals</th>\n",
       "      <th>season_2</th>\n",
       "      <th>season_3</th>\n",
       "      <th>season_4</th>\n",
       "      <th>month_2</th>\n",
       "      <th>month_3</th>\n",
       "      <th>month_4</th>\n",
       "      <th>month_5</th>\n",
       "      <th>month_6</th>\n",
       "      <th>month_7</th>\n",
       "      <th>month_8</th>\n",
       "      <th>month_9</th>\n",
       "      <th>month_10</th>\n",
       "      <th>month_11</th>\n",
       "      <th>month_12</th>\n",
       "      <th>day_of_week_1</th>\n",
       "      <th>day_of_week_2</th>\n",
       "      <th>day_of_week_3</th>\n",
       "      <th>day_of_week_4</th>\n",
       "      <th>day_of_week_5</th>\n",
       "      <th>day_of_week_6</th>\n",
       "      <th>weather_2</th>\n",
       "      <th>weather_3</th>\n",
       "      <th>temp^2</th>\n",
       "      <th>temp^3</th>\n",
       "      <th>temp^4</th>\n",
       "      <th>atemp^2</th>\n",
       "      <th>atemp^3</th>\n",
       "      <th>atemp^4</th>\n",
       "      <th>humidity^2</th>\n",
       "      <th>humidity^3</th>\n",
       "      <th>humidity^4</th>\n",
       "      <th>windspeed^2</th>\n",
       "      <th>windspeed^3</th>\n",
       "      <th>windspeed^4</th>\n",
       "      <th>month12_temp</th>\n",
       "      <th>workday_weather</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.623798</td>\n",
       "      <td>0.650106</td>\n",
       "      <td>0.920664</td>\n",
       "      <td>-0.928758</td>\n",
       "      <td>6073.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.389124</td>\n",
       "      <td>0.242735</td>\n",
       "      <td>0.151418</td>\n",
       "      <td>0.422637</td>\n",
       "      <td>0.274759</td>\n",
       "      <td>0.178622</td>\n",
       "      <td>0.847622</td>\n",
       "      <td>0.780375</td>\n",
       "      <td>0.718464</td>\n",
       "      <td>0.862591</td>\n",
       "      <td>-0.801139</td>\n",
       "      <td>0.744064</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.180310</td>\n",
       "      <td>-0.054759</td>\n",
       "      <td>0.696852</td>\n",
       "      <td>-0.213502</td>\n",
       "      <td>6606.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.032512</td>\n",
       "      <td>-0.005862</td>\n",
       "      <td>0.001057</td>\n",
       "      <td>0.002998</td>\n",
       "      <td>-0.000164</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.485603</td>\n",
       "      <td>0.338393</td>\n",
       "      <td>0.235810</td>\n",
       "      <td>0.045583</td>\n",
       "      <td>-0.009732</td>\n",
       "      <td>0.002078</td>\n",
       "      <td>-0.180310</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.802489</td>\n",
       "      <td>0.851495</td>\n",
       "      <td>-0.448383</td>\n",
       "      <td>0.803926</td>\n",
       "      <td>7363.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.643989</td>\n",
       "      <td>0.516794</td>\n",
       "      <td>0.414722</td>\n",
       "      <td>0.725044</td>\n",
       "      <td>0.617372</td>\n",
       "      <td>0.525689</td>\n",
       "      <td>0.201047</td>\n",
       "      <td>-0.090146</td>\n",
       "      <td>0.040420</td>\n",
       "      <td>0.646297</td>\n",
       "      <td>0.519575</td>\n",
       "      <td>0.417699</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.520492</td>\n",
       "      <td>-1.565182</td>\n",
       "      <td>-0.332113</td>\n",
       "      <td>-0.269099</td>\n",
       "      <td>2431.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.311895</td>\n",
       "      <td>-3.515217</td>\n",
       "      <td>5.344859</td>\n",
       "      <td>2.449794</td>\n",
       "      <td>-3.834373</td>\n",
       "      <td>6.001490</td>\n",
       "      <td>0.110299</td>\n",
       "      <td>-0.036632</td>\n",
       "      <td>0.012166</td>\n",
       "      <td>0.072414</td>\n",
       "      <td>-0.019487</td>\n",
       "      <td>0.005244</td>\n",
       "      <td>-1.520492</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.534453</td>\n",
       "      <td>0.348021</td>\n",
       "      <td>1.975789</td>\n",
       "      <td>-1.199027</td>\n",
       "      <td>1996.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.285640</td>\n",
       "      <td>0.152661</td>\n",
       "      <td>0.081590</td>\n",
       "      <td>0.121119</td>\n",
       "      <td>0.042152</td>\n",
       "      <td>0.014670</td>\n",
       "      <td>3.903744</td>\n",
       "      <td>7.712976</td>\n",
       "      <td>15.239217</td>\n",
       "      <td>1.437666</td>\n",
       "      <td>-1.723801</td>\n",
       "      <td>2.066885</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   holiday  workingday      temp     atemp  humidity  windspeed  rentals  \\\n",
       "0      0.0         1.0  0.623798  0.650106  0.920664  -0.928758   6073.0   \n",
       "1      0.0         1.0 -0.180310 -0.054759  0.696852  -0.213502   6606.0   \n",
       "2      0.0         1.0  0.802489  0.851495 -0.448383   0.803926   7363.0   \n",
       "3      0.0         0.0 -1.520492 -1.565182 -0.332113  -0.269099   2431.0   \n",
       "4      0.0         1.0  0.534453  0.348021  1.975789  -1.199027   1996.0   \n",
       "\n",
       "   season_2  season_3  season_4  month_2  month_3  month_4  month_5  month_6  \\\n",
       "0         1         0         0        0        0        0        1        0   \n",
       "1         0         0         1        0        0        0        0        0   \n",
       "2         1         0         0        0        0        0        0        1   \n",
       "3         0         0         1        0        0        0        0        0   \n",
       "4         0         1         0        0        0        0        0        0   \n",
       "\n",
       "   month_7  month_8  month_9  month_10  month_11  month_12  day_of_week_1  \\\n",
       "0        0        0        0         0         0         0              0   \n",
       "1        0        0        0         0         0         1              0   \n",
       "2        0        0        0         0         0         0              0   \n",
       "3        0        0        0         0         0         1              0   \n",
       "4        0        0        1         0         0         0              0   \n",
       "\n",
       "   day_of_week_2  day_of_week_3  day_of_week_4  day_of_week_5  day_of_week_6  \\\n",
       "0              1              0              0              0              0   \n",
       "1              1              0              0              0              0   \n",
       "2              0              0              1              0              0   \n",
       "3              0              0              0              0              0   \n",
       "4              0              1              0              0              0   \n",
       "\n",
       "   weather_2  weather_3    temp^2    temp^3    temp^4   atemp^2   atemp^3  \\\n",
       "0          1          0  0.389124  0.242735  0.151418  0.422637  0.274759   \n",
       "1          0          0  0.032512 -0.005862  0.001057  0.002998 -0.000164   \n",
       "2          0          0  0.643989  0.516794  0.414722  0.725044  0.617372   \n",
       "3          0          0  2.311895 -3.515217  5.344859  2.449794 -3.834373   \n",
       "4          0          1  0.285640  0.152661  0.081590  0.121119  0.042152   \n",
       "\n",
       "    atemp^4  humidity^2  humidity^3  humidity^4  windspeed^2  windspeed^3  \\\n",
       "0  0.178622    0.847622    0.780375    0.718464     0.862591    -0.801139   \n",
       "1  0.000009    0.485603    0.338393    0.235810     0.045583    -0.009732   \n",
       "2  0.525689    0.201047   -0.090146    0.040420     0.646297     0.519575   \n",
       "3  6.001490    0.110299   -0.036632    0.012166     0.072414    -0.019487   \n",
       "4  0.014670    3.903744    7.712976   15.239217     1.437666    -1.723801   \n",
       "\n",
       "   windspeed^4  month12_temp  workday_weather  \n",
       "0     0.744064      0.000000                0  \n",
       "1     0.002078     -0.180310                1  \n",
       "2     0.417699      0.000000                1  \n",
       "3     0.005244     -1.520492                0  \n",
       "4     2.066885      0.000000                0  "
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(331, 42) (331, 1) (400, 42) (400, 1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>holiday</th>\n",
       "      <th>workingday</th>\n",
       "      <th>temp</th>\n",
       "      <th>atemp</th>\n",
       "      <th>humidity</th>\n",
       "      <th>windspeed</th>\n",
       "      <th>rentals</th>\n",
       "      <th>season_2</th>\n",
       "      <th>season_3</th>\n",
       "      <th>season_4</th>\n",
       "      <th>month_2</th>\n",
       "      <th>month_3</th>\n",
       "      <th>month_4</th>\n",
       "      <th>month_5</th>\n",
       "      <th>month_6</th>\n",
       "      <th>month_7</th>\n",
       "      <th>month_8</th>\n",
       "      <th>month_9</th>\n",
       "      <th>month_10</th>\n",
       "      <th>month_11</th>\n",
       "      <th>month_12</th>\n",
       "      <th>day_of_week_1</th>\n",
       "      <th>day_of_week_2</th>\n",
       "      <th>day_of_week_3</th>\n",
       "      <th>day_of_week_4</th>\n",
       "      <th>day_of_week_5</th>\n",
       "      <th>day_of_week_6</th>\n",
       "      <th>weather_2</th>\n",
       "      <th>weather_3</th>\n",
       "      <th>temp^2</th>\n",
       "      <th>temp^3</th>\n",
       "      <th>temp^4</th>\n",
       "      <th>atemp^2</th>\n",
       "      <th>atemp^3</th>\n",
       "      <th>atemp^4</th>\n",
       "      <th>humidity^2</th>\n",
       "      <th>humidity^3</th>\n",
       "      <th>humidity^4</th>\n",
       "      <th>windspeed^2</th>\n",
       "      <th>windspeed^3</th>\n",
       "      <th>windspeed^4</th>\n",
       "      <th>month12_temp</th>\n",
       "      <th>workday_weather</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.623798</td>\n",
       "      <td>0.650106</td>\n",
       "      <td>0.920664</td>\n",
       "      <td>-0.928758</td>\n",
       "      <td>6073.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.389124</td>\n",
       "      <td>0.242735</td>\n",
       "      <td>0.151418</td>\n",
       "      <td>0.422637</td>\n",
       "      <td>0.274759</td>\n",
       "      <td>0.178622</td>\n",
       "      <td>0.847622</td>\n",
       "      <td>0.780375</td>\n",
       "      <td>0.718464</td>\n",
       "      <td>0.862591</td>\n",
       "      <td>-0.801139</td>\n",
       "      <td>0.744064</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.180310</td>\n",
       "      <td>-0.054759</td>\n",
       "      <td>0.696852</td>\n",
       "      <td>-0.213502</td>\n",
       "      <td>6606.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.032512</td>\n",
       "      <td>-0.005862</td>\n",
       "      <td>0.001057</td>\n",
       "      <td>0.002998</td>\n",
       "      <td>-0.000164</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.485603</td>\n",
       "      <td>0.338393</td>\n",
       "      <td>0.235810</td>\n",
       "      <td>0.045583</td>\n",
       "      <td>-0.009732</td>\n",
       "      <td>0.002078</td>\n",
       "      <td>-0.180310</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.802489</td>\n",
       "      <td>0.851495</td>\n",
       "      <td>-0.448383</td>\n",
       "      <td>0.803926</td>\n",
       "      <td>7363.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.643989</td>\n",
       "      <td>0.516794</td>\n",
       "      <td>0.414722</td>\n",
       "      <td>0.725044</td>\n",
       "      <td>0.617372</td>\n",
       "      <td>0.525689</td>\n",
       "      <td>0.201047</td>\n",
       "      <td>-0.090146</td>\n",
       "      <td>0.040420</td>\n",
       "      <td>0.646297</td>\n",
       "      <td>0.519575</td>\n",
       "      <td>0.417699</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.520492</td>\n",
       "      <td>-1.565182</td>\n",
       "      <td>-0.332113</td>\n",
       "      <td>-0.269099</td>\n",
       "      <td>2431.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.311895</td>\n",
       "      <td>-3.515217</td>\n",
       "      <td>5.344859</td>\n",
       "      <td>2.449794</td>\n",
       "      <td>-3.834373</td>\n",
       "      <td>6.001490</td>\n",
       "      <td>0.110299</td>\n",
       "      <td>-0.036632</td>\n",
       "      <td>0.012166</td>\n",
       "      <td>0.072414</td>\n",
       "      <td>-0.019487</td>\n",
       "      <td>0.005244</td>\n",
       "      <td>-1.520492</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.534453</td>\n",
       "      <td>0.348021</td>\n",
       "      <td>1.975789</td>\n",
       "      <td>-1.199027</td>\n",
       "      <td>1996.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.285640</td>\n",
       "      <td>0.152661</td>\n",
       "      <td>0.081590</td>\n",
       "      <td>0.121119</td>\n",
       "      <td>0.042152</td>\n",
       "      <td>0.014670</td>\n",
       "      <td>3.903744</td>\n",
       "      <td>7.712976</td>\n",
       "      <td>15.239217</td>\n",
       "      <td>1.437666</td>\n",
       "      <td>-1.723801</td>\n",
       "      <td>2.066885</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   holiday  workingday      temp     atemp  humidity  windspeed  rentals  \\\n",
       "0      0.0         1.0  0.623798  0.650106  0.920664  -0.928758   6073.0   \n",
       "1      0.0         1.0 -0.180310 -0.054759  0.696852  -0.213502   6606.0   \n",
       "2      0.0         1.0  0.802489  0.851495 -0.448383   0.803926   7363.0   \n",
       "3      0.0         0.0 -1.520492 -1.565182 -0.332113  -0.269099   2431.0   \n",
       "4      0.0         1.0  0.534453  0.348021  1.975789  -1.199027   1996.0   \n",
       "\n",
       "   season_2  season_3  season_4  month_2  month_3  month_4  month_5  month_6  \\\n",
       "0         1         0         0        0        0        0        1        0   \n",
       "1         0         0         1        0        0        0        0        0   \n",
       "2         1         0         0        0        0        0        0        1   \n",
       "3         0         0         1        0        0        0        0        0   \n",
       "4         0         1         0        0        0        0        0        0   \n",
       "\n",
       "   month_7  month_8  month_9  month_10  month_11  month_12  day_of_week_1  \\\n",
       "0        0        0        0         0         0         0              0   \n",
       "1        0        0        0         0         0         1              0   \n",
       "2        0        0        0         0         0         0              0   \n",
       "3        0        0        0         0         0         1              0   \n",
       "4        0        0        1         0         0         0              0   \n",
       "\n",
       "   day_of_week_2  day_of_week_3  day_of_week_4  day_of_week_5  day_of_week_6  \\\n",
       "0              1              0              0              0              0   \n",
       "1              1              0              0              0              0   \n",
       "2              0              0              1              0              0   \n",
       "3              0              0              0              0              0   \n",
       "4              0              1              0              0              0   \n",
       "\n",
       "   weather_2  weather_3    temp^2    temp^3    temp^4   atemp^2   atemp^3  \\\n",
       "0          1          0  0.389124  0.242735  0.151418  0.422637  0.274759   \n",
       "1          0          0  0.032512 -0.005862  0.001057  0.002998 -0.000164   \n",
       "2          0          0  0.643989  0.516794  0.414722  0.725044  0.617372   \n",
       "3          0          0  2.311895 -3.515217  5.344859  2.449794 -3.834373   \n",
       "4          0          1  0.285640  0.152661  0.081590  0.121119  0.042152   \n",
       "\n",
       "    atemp^4  humidity^2  humidity^3  humidity^4  windspeed^2  windspeed^3  \\\n",
       "0  0.178622    0.847622    0.780375    0.718464     0.862591    -0.801139   \n",
       "1  0.000009    0.485603    0.338393    0.235810     0.045583    -0.009732   \n",
       "2  0.525689    0.201047   -0.090146    0.040420     0.646297     0.519575   \n",
       "3  6.001490    0.110299   -0.036632    0.012166     0.072414    -0.019487   \n",
       "4  0.014670    3.903744    7.712976   15.239217     1.437666    -1.723801   \n",
       "\n",
       "   windspeed^4  month12_temp  workday_weather  \n",
       "0     0.744064      0.000000                0  \n",
       "1     0.002078     -0.180310                1  \n",
       "2     0.417699      0.000000                1  \n",
       "3     0.005244     -1.520492                0  \n",
       "4     2.066885      0.000000                0  "
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create an array of values for our regression\n",
    "y_train = train_data['rentals'].values\n",
    "X_train = train_data[['holiday', 'workingday', 'temp', 'atemp', 'humidity', 'windspeed', 'season_2', \n",
    "                 'season_3', 'season_4', 'month_2', 'month_3', 'month_4', 'month_5', 'month_6',\n",
    "                 'month_7', 'month_8', 'month_9', 'month_10', 'month_11', 'month_12', 'day_of_week_1', \n",
    "                 'day_of_week_2', 'day_of_week_3', 'day_of_week_4', 'day_of_week_5', 'day_of_week_6', 'weather_2', \n",
    "                 'weather_3', 'temp^2', 'temp^3', 'temp^4', 'atemp^2', 'atemp^3', 'atemp^4', 'humidity^2', 'humidity^3', \n",
    "                 'humidity^4', 'windspeed^2', 'windspeed^3', 'windspeed^4', 'workday_weather', 'month12_temp']].values\n",
    "\n",
    "y_test = test_data['rentals'].values\n",
    "X_test = test_data[['holiday', 'workingday', 'temp', 'atemp', 'humidity', 'windspeed', 'season_2', \n",
    "                 'season_3', 'season_4', 'month_2', 'month_3', 'month_4', 'month_5', 'month_6',\n",
    "                 'month_7', 'month_8', 'month_9', 'month_10', 'month_11', 'month_12', 'day_of_week_1', \n",
    "                 'day_of_week_2', 'day_of_week_3', 'day_of_week_4', 'day_of_week_5', 'day_of_week_6', 'weather_2', \n",
    "                 'weather_3', 'temp^2', 'temp^3', 'temp^4', 'atemp^2', 'atemp^3', 'atemp^4', 'humidity^2', 'humidity^3', \n",
    "                 'humidity^4', 'windspeed^2', 'windspeed^3', 'windspeed^4', 'workday_weather', 'month12_temp']].values\n",
    "\n",
    "y_train = y_train.reshape(len(y_train), 1)\n",
    "y_test = y_test.reshape(len(y_test), 1)\n",
    "\n",
    "print(X_train.shape, y_train.shape, X_test.shape , y_test.shape)\n",
    "\n",
    "train_data.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The equation of the regression plane is: [ 5002.87217487] + [[ -1.74548786e+02   2.51998375e+02   7.95078743e+02   8.81715282e+02\n",
      "   -6.76351913e+02  -4.47079697e+02   7.63637875e+02   1.56427958e+03\n",
      "    1.49316259e+03  -3.25739496e+02  -3.11735875e+02  -4.29582296e+02\n",
      "   -1.02485943e+03  -1.46665414e+03  -1.42767388e+03  -1.72063755e+03\n",
      "   -1.05590929e+03  -8.82233606e+02  -7.73613283e+02  -4.80922153e+02\n",
      "   -1.07457193e+02  -1.48195150e+02   1.29095641e+02   1.21349048e+01\n",
      "    1.91871387e+02   4.62448641e+02   1.81873517e+02  -9.23040593e+02\n",
      "   -1.80185812e+03   1.66279577e+00  -4.47557316e+01   1.17067291e+03\n",
      "   -2.98989072e+02  -2.10470354e+01  -5.65186328e+01  -1.50310861e+01\n",
      "   -2.39876686e+01  -3.78573222e+01   4.45758104e+01  -1.93589160e+01\n",
      "    1.65026491e+02   4.44106026e+01]] * x\n",
      "The train MSE is 1232388.953387242, the test MSE is 3131990.9875292666\n",
      "The train R^2 is 0.6699675993036875, the test R^2 is 0.2829779330240658\n"
     ]
    }
   ],
   "source": [
    "# fit model with the additional interaction terms, compute metrics\n",
    "lm = LinearRegression(fit_intercept=True)\n",
    "lm.fit(X_train, y_train)\n",
    "\n",
    "y_pred = lm.predict(X_test)\n",
    "\n",
    "print('The equation of the regression plane is: {} + {} * x'.format(lm.intercept_, lm.coef_))\n",
    "\n",
    "train_MSE= np.mean((y_train - lm.predict(X_train))**2)\n",
    "test_MSE= np.mean((y_test - lm.predict(X_test))**2)\n",
    "print('The train MSE is {}, the test MSE is {}'.format(train_MSE, test_MSE))\n",
    "\n",
    "train_R_sq = lm.score(X_train, y_train)\n",
    "test_R_sq = lm.score(X_test, y_test)\n",
    "print('The train R^2 is {}, the test R^2 is {}'.format(train_R_sq, test_R_sq))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part (h): PCA to deal with high dimensionality\n",
    "\n",
    "We would like to fit a model to include all main effects, polynomial terms up to the $4^{th}$ order, and all interactions between all possible predictors and polynomial terms (not including the interactions between $X^1_j$, $X^2_j$, $X^3_j$, and $X^4_j$ as they would just create higher order polynomial terms).  \n",
    "\n",
    "- Create an expanded training set including all the desired terms mentioned above.  What are the dimensions of this 'design matrix' of all the predictor variables?   What are the issues with attempting to fit a regression model using all of these predictors?\n",
    "\n",
    "- Instead of using the usual approaches for model selection, let's instead use principal components analysis (PCA) to fit the model.  First, create the principal component vectors in python (consider: should you normalize first?).  Then fit 5 different regression models: (1) using just the first PCA vector, (2) using the first two PCA vectors, (3) using the first three PCA vectors, etc...  Briefly summarize how these models compare in the training set.\n",
    "\n",
    "- Use the test set to decide which of the 5 models above is best to predict out of sample.  How does this model compare to the previous models you've fit?  What are the interpretations of this model's coefficients?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The additional dimensions are interactions between the one-hot variables. While this analysis can capture some\n",
    "# complex relationships, we only have 331 data items to begin with, and as the combinations of different parameters\n",
    "# grows, you can quickly run out of data! When the number of factors exceeds the number of data items, the regression is\n",
    "# unspecified. The purpose of PCA is to reduce the number of factors into groups of the most influential ones.\n",
    "\n",
    "# The first three PCA components accounted for 46%, 21% and 19% of the variation in our data. \n",
    "\n",
    "# We decided to stick with the predictors without the interaction terms, because we did not find them to be statistically\n",
    "# significant.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained variance ratio: [ 0.46123346]\n",
      "[[ -4.56350851e-04  -1.30637936e-03  -2.43585426e-02  -2.62500820e-02\n",
      "   -1.88305024e-02   5.68188099e-02  -1.97999683e-03  -5.74257761e-03\n",
      "   -3.80104140e-03   6.12190924e-03   3.60177162e-03   9.51634682e-04\n",
      "   -2.64119912e-03  -1.34425553e-03  -2.74732289e-03  -1.58700340e-03\n",
      "   -2.14616037e-03  -1.70033278e-03  -9.03383335e-04  -2.42813544e-04\n",
      "    5.58731456e-04   1.48630748e-03  -1.83413829e-03  -2.63235240e-03\n",
      "    6.58721549e-04   1.48868776e-03   7.67228134e-04   5.60494234e-04\n",
      "    1.16305876e-02  -6.04297300e-02   7.30587192e-02   1.87160516e-02\n",
      "   -8.02179436e-02   1.29792502e-01   3.22434422e-02  -5.44238937e-02\n",
      "    1.12441696e-01   1.27833825e-01   3.38050800e-01   9.02754649e-01]]\n",
      "Explained variance ratio: [ 0.46123346  0.21350874]\n",
      "Explained variance ratio: [ 0.46123346  0.21350874  0.19390969]\n"
     ]
    }
   ],
   "source": [
    "#create polynomial features matrix\n",
    "polynomial_features = PolynomialFeatures(degree=1, interaction_only=True, include_bias=True)\n",
    "poly = polynomial_features.fit_transform(X_train)\n",
    "\n",
    "\n",
    "#Using too many features can create an overfitting problem, particularly if they are of high degree - the training\n",
    "#process will fit to the noise! Here, the degree of the factors isn't high (limited to 1), but with enough factors,\n",
    "#you could wind up with more factors than you have data items, in which case the model is not specified.\n",
    "\n",
    "\n",
    "pca1 = PCA(n_components=1)\n",
    "pca1.fit(X_train)\n",
    "X_train_pca1 = pca1.transform(X_train)\n",
    "X_test_pca1 = pca1.transform(X_test)\n",
    "print('Explained variance ratio:', pca1.explained_variance_ratio_)\n",
    "\n",
    "pca2 = PCA(n_components=2)\n",
    "pca2.fit(X_train)\n",
    "X_train_pca2 = pca2.transform(X_train)\n",
    "X_test_pca2 = pca2.transform(X_test)\n",
    "print('Explained variance ratio:', pca2.explained_variance_ratio_)\n",
    "\n",
    "pca3 = PCA(n_components=3)\n",
    "pca3.fit(X_train)\n",
    "X_train_pca3 = pca3.transform(X_train)\n",
    "X_test_pca3 = pca3.transform(X_test)\n",
    "print('Explained variance ratio:', pca3.explained_variance_ratio_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA w 1 component Test R^2: -0.058160712318159336\n",
      "PCA w 2 component Test R^2: 0.012089582480906969\n",
      "PCA w 3 component Test R^2: -0.020903459867501306\n"
     ]
    }
   ],
   "source": [
    "regression_model_pca1 = LinearRegression(fit_intercept=True)\n",
    "regression_model_pca1.fit(X_train_pca1, y_train)\n",
    "y_pred_1 = regression_model_pca1.predict(X_test_pca1)\n",
    "score_1 = r2_score(y_test, y_pred_1)\n",
    "\n",
    "print('PCA w 1 component Test R^2: {}'.format(score_1))\n",
    "\n",
    "regression_model_pca2 = LinearRegression(fit_intercept=True)\n",
    "regression_model_pca2.fit(X_train_pca2, y_train)\n",
    "\n",
    "print('PCA w 2 component Test R^2: {}'.format(regression_model_pca2.score(X_test_pca2, y_test)))\n",
    "\n",
    "regression_model_pca3 = LinearRegression(fit_intercept=True)\n",
    "regression_model_pca3.fit(X_train_pca3, y_train)\n",
    "\n",
    "print('PCA w 3 component Test R^2: {}'.format(regression_model_pca3.score(X_test_pca3, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABJgAAAFNCAYAAABMn9WLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f9dbd7c4a58>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(20, 5))\n",
    "\n",
    "ax[0].scatter(X_train_pca3[:, 0], X_train_pca3[:, 1], color='blue', alpha=0.2, label='train R^2')\n",
    "\n",
    "ax[0].set_title('Dimension Reduced Data')\n",
    "ax[0].set_xlabel('1st Principal Component')\n",
    "ax[0].set_ylabel('2nd Principal Component')\n",
    "\n",
    "ax[1].scatter(X_train_pca3[:, 1], X_train_pca3[:, 2], color='red', alpha=0.2, label='train R^2')\n",
    "\n",
    "ax[1].set_title('Dimension Reduced Data')\n",
    "ax[1].set_xlabel('2nd Principal Component')\n",
    "ax[1].set_ylabel('3rd Principal Component')\n",
    "\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part (i): Beyond Squared Error\n",
    "\n",
    "We have seen in class that the multiple linear regression method optimizes the Mean Squared Error (MSE) on the training set. Consider the following alternate evaluation metric, referred to as the Root Mean Squared Logarthmic Error (RMSLE):\n",
    "\n",
    "$$\n",
    "\\sqrt{\\frac{1}{n}\\sum_{i=1}^n (log(y_i+1) - log(\\hat{y}_i+1))^2}.\n",
    "$$\n",
    "\n",
    "The *lower* the RMSLE the *better* is the performance of a model. The RMSLE penalizes errors on smaller responses more heavily than errors on larger responses. For example, the RMSLE penalizes a prediction of $\\hat{y} = 15$ for a true response of $y=10$ more heavily than a prediction of $\\hat{y} = 105$ for a true response of $100$, though the difference in predicted and true responses are the same in both cases. \n",
    "\n",
    "This is a natural evaluation metric for bike share demand prediction, as in this application, it is more important that the prediction model is accurate on days where the demand is low (so that the few customers who arrive are served satisfactorily), compared to days on which the demand is high (when it is less damaging to lose out on some customers).\n",
    "\n",
    "The following code computes the RMSLE for you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#--------  rmsle\n",
    "# A function for evaluating Root Mean Squared Logarithmic Error (RMSLE)\n",
    "# of the linear regression model on a data set\n",
    "# Input: \n",
    "#      y_test (n x 1 array of response variable vals in testing data)\n",
    "#      y_pred (n x 1 array of response variable vals in testing data)\n",
    "# Return: \n",
    "#      RMSLE (float) \n",
    "\n",
    "def rmsle(y, y_pred):     \n",
    "    where_are_NaNs = np.isnan(np.log(y_pred+1))\n",
    "    y_pred[where_are_NaNs] = 0\n",
    "    rmsle_ = np.sqrt(np.mean(np.square(np.log(y+1) - np.log(y_pred+1))))\n",
    "    print(\"RMSLE is \", rmsle_)\n",
    "    \n",
    "    return rmsle_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the above code to compute the training and test RMSLE for the polynomial regression model you fit in Part (g). \n",
    "\n",
    "You are required to develop a strategy to fit a regression model by optimizing the RMSLE on the training set. Give a justification for your proposed approach. Does the model fitted using your approach yield lower train RMSLE than the model in Part (g)? How about the test RMSLE of the new model? \n",
    "\n",
    "**Note:** We do not require you to implement a new regression solver for RMSLE. Instead, we ask you to think about ways to use existing built-in functions to fit a model that performs well on RMSLE. Your regression model may use the same polynomial terms used in Part (g)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:\n",
      "RMSLE is  0.721457659701\n",
      "Test:\n",
      "RMSLE is  0.857618314536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:11: RuntimeWarning: invalid value encountered in log\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    }
   ],
   "source": [
    "print(\"Train:\")\n",
    "RMSLE = rmsle(y_train, lm.predict(X_train))\n",
    "print(\"Test:\")\n",
    "RMSLE = rmsle(y_test, lm.predict(X_test))\n",
    "\n",
    "# Our best guess is that we would use RMSLE to estimate a kind of lambda factor. The idea is that it is a logarithmic measure and\n",
    "# can measure actual versus predicted values according to this relationship: log((pi+1)/(ai+1)), where pi is the\n",
    "# predicted value and ai is the actual. It can be used to penalize under-estimates on a different basis than\n",
    "# over-estimates, as might be the case when the suppliers of the bicycles being studied here are trying to decide how\n",
    "# many bikes to stock - if people expect to face a stock-out because it is a busy day, you're not so worried about\n",
    "# estimating demand, but if you underestimate demand on a lighter day, you could make a number of customers angry.\n",
    "#\n",
    "# It should be noted that the lambda we are suggesting above would a *multiplier* to *increase* estimated rentals\n",
    "# givne a high RMSLE than the way we've used it elsewhere here, as a reducer. And it may be outside the regression\n",
    "# equation entirely, but rather as a \"post-processing\" multiplier where you estimate rentals given parameters provided\n",
    "# by the regression, AND THEN apply the multiplier suggested by the RMSLE.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "your answers here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part (j): Dealing with Erroneous Labels\n",
    "\n",
    "Due to occasional system crashes, some of the bike counts reported in the data set have been recorded manually. These counts are not very unreliable and are prone to errors. It is known that roughly 5% of the labels in the training set are erroneous (i.e. can be arbitrarily different from the true counts), while all the labels in the test set were confirmed to be accurate. Unfortunately, the identities of the erroneous records in the training set are not available. Can this information about presence of 5% errors in the training set labels (without details about the specific identities of the erroneous rows) be used to improve the performance of the model in Part (g)? Note that we are interested in improving the $R^2$ performance of the model on the test set (not the training $R^2$ score). \n",
    "\n",
    "As a final task, we require you to come up with a strategy to fit a regression model, taking into account the errors in the training set labels. Explain the intuition behind your approach (we do not expect a detailed mathematical justification). Use your approach to fit a regression model on the training set, and compare its test $R^2$ with the model in Part (g).\n",
    "\n",
    "**Note:** Again, we do not require you to implement a new regression solver for handling erroneous labels. It is sufficient that you to come up with an approach that uses existing built-in functions. Your regression model may use the same polynomial terms used in Part (g)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(331, 40) (331, 1) (400, 40) (400, 1)\n",
      "The equation of the regression plane is: [ 5035.27125887] + [[ -189.7675006    351.27394405   771.48662326   897.2756023   -668.91446319\n",
      "   -446.50850455   766.43070371  1578.75436674  1523.2288234   -325.06857397\n",
      "   -304.84911267  -418.02446177 -1037.20424547 -1456.18565573\n",
      "  -1416.98816779 -1715.93889094 -1073.40080145  -925.87103867\n",
      "   -825.53284158  -555.66756465   -93.32647706  -133.42791146   147.7312741\n",
      "     30.59243395   209.93712392   471.0834343     59.01188326\n",
      "  -1043.99967412 -1811.01797233     8.60775572   -45.191024    1175.50049569\n",
      "   -303.93578541   -20.76855912   -53.67085686   -16.05763165   -24.8367323\n",
      "    -34.16534669    44.8338792    -20.17694855]] * x\n",
      "The train MSE is 1233551.6134555764, the test MSE is 3157061.4234926915\n",
      "The train R^2 is 0.6696562402214016, the test R^2 is 0.27723843508615387\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "\n",
    "\n",
    "for x in range(0, 331):\n",
    "    if (train_data.get_value(x, 'month_2') == 1 or train_data.get_value(x, 'month_3') == 1):\n",
    "        train_data.set_value('season_2', x, 0)\n",
    "        train_data.set_value('season_3', x, 0)\n",
    "        train_data.set_value('season_4', x, 0)\n",
    "    elif (train_data.get_value(x, 'month_4') == 1 or train_data.get_value(x, 'month_5') == 1 or train_data.get_value(x, 'month_6') == 1):\n",
    "        train_data.set_value('season_2', x, 1)\n",
    "        train_data.set_value('season_3', x, 0)\n",
    "        train_data.set_value('season_4', x, 0)\n",
    "    elif (train_data.get_value(x, 'month_7') == 1 or train_data.get_value(x, 'month_8') == 1 or train_data.get_value(x, 'month_9') == 1):\n",
    "        train_data.set_value('season_2', x, 0)\n",
    "        train_data.set_value('season_3', x, 1)\n",
    "        train_data.set_value('season_4', x, 0)\n",
    "    elif (train_data.get_value(x, 'month_10') == 1 or train_data.get_value(x, 'month_11') == 1 or train_data.get_value(x, 'month_12') == 1):\n",
    "        train_data.set_value('season_2', x, 0)\n",
    "        train_data.set_value('season_3', x, 0)\n",
    "        train_data.set_value('season_4', x, 1)\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "y_train = train_data['rentals'].values\n",
    "y_train = y_train[0:-3]\n",
    "y_train = y_train.reshape(len(y_train), 1)\n",
    "\n",
    "X_train = train_data[['holiday', 'workingday', 'temp', 'atemp', 'humidity', 'windspeed', 'season_2', \n",
    "                 'season_3', 'season_4', 'month_2', 'month_3', 'month_4', 'month_5', 'month_6',\n",
    "                 'month_7', 'month_8', 'month_9', 'month_10', 'month_11', 'month_12', 'day_of_week_1', \n",
    "                 'day_of_week_2', 'day_of_week_3', 'day_of_week_4', 'day_of_week_5', 'day_of_week_6', 'weather_2', \n",
    "                 'weather_3', 'temp^2', 'temp^3', 'temp^4', 'atemp^2', 'atemp^3', 'atemp^4', 'humidity^2', 'humidity^3', \n",
    "                 'humidity^4', 'windspeed^2', 'windspeed^3', 'windspeed^4']].values\n",
    "X_train = X_train[0:-3]\n",
    "y_test = test_data['rentals'].values\n",
    "X_test = test_data[['holiday', 'workingday', 'temp', 'atemp', 'humidity', 'windspeed', 'season_2', \n",
    "                 'season_3', 'season_4', 'month_2', 'month_3', 'month_4', 'month_5', 'month_6',\n",
    "                 'month_7', 'month_8', 'month_9', 'month_10', 'month_11', 'month_12', 'day_of_week_1', \n",
    "                 'day_of_week_2', 'day_of_week_3', 'day_of_week_4', 'day_of_week_5', 'day_of_week_6', 'weather_2', \n",
    "                 'weather_3', 'temp^2', 'temp^3', 'temp^4', 'atemp^2', 'atemp^3', 'atemp^4', 'humidity^2', 'humidity^3', \n",
    "                 'humidity^4', 'windspeed^2', 'windspeed^3', 'windspeed^4']].values\n",
    "\n",
    "y_test = y_test.reshape(len(y_test), 1)\n",
    "\n",
    "print(X_train.shape, y_train.shape, X_test.shape , y_test.shape)\n",
    "\n",
    "lm = LinearRegression(fit_intercept=True)\n",
    "lm.fit(X_train, y_train)\n",
    "\n",
    "y_pred = lm.predict(X_test)\n",
    "\n",
    "print('The equation of the regression plane is: {} + {} * x'.format(lm.intercept_, lm.coef_))\n",
    "\n",
    "train_MSE= np.mean((y_train - lm.predict(X_train))**2)\n",
    "test_MSE= np.mean((y_test - lm.predict(X_test))**2)\n",
    "print('The train MSE is {}, the test MSE is {}'.format(train_MSE, test_MSE))\n",
    "\n",
    "train_R_sq = lm.score(X_train, y_train)\n",
    "test_R_sq = lm.score(X_test, y_test)\n",
    "print('The train R^2 is {}, the test R^2 is {}'.format(train_R_sq, test_R_sq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" \\n\\nAnalysis\\n\\n1. I would start off with checking the season and month figures for inconsistencies. \\n2. I would also check for consistencies between the working day binary parameter and the binary day flags.\\n3. I would check for sanity in the deltas between atemp and temp\\n\\nWe visually inspected all three, and found a number of errors in 1, i.e., a season number would be ascribed to a\\ntuple with a month that didn't match. We were only able to find one error for #2, and could not find any errors for #3.\\n(We visually inspected deltas between temp and atemp and didn't find anything noteworthy.)\\n\\nOur solution: assign season automatically depending on month for the entire training data set.\\n\\nIt turned out that our r-squared was actually REDUCED \\n\\n\""
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' \n",
    "\n",
    "Analysis\n",
    "\n",
    "1. I would start off with checking the season and month figures for inconsistencies. \n",
    "2. I would also check for consistencies between the working day binary parameter and the binary day flags.\n",
    "3. I would check for sanity in the deltas between atemp and temp\n",
    "\n",
    "We visually inspected all three, and found a number of errors in 1, i.e., a season number would be ascribed to a\n",
    "tuple with a month that didn't match. We were only able to find one error for #2, and could not find any errors for #3.\n",
    "(We visually inspected deltas between temp and atemp and didn't find anything noteworthy.)\n",
    "\n",
    "Our solution: assign season automatically depending on month for the entire training data set.\n",
    "\n",
    "It turned out that our r-squared was actually REDUCED from 0.282 to 0.277! Cleaning the data did not impact our\n",
    "results in any meaningful way. However, it may be that there is some more creative way to clean the data that we\n",
    "simply did not think of, so we leave the door open to the possibility of obtaining more meaningful results.\n",
    "\n",
    "'''\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
